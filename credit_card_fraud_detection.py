# -*- coding: utf-8 -*-
"""credit_card_fraud_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fs1U7dDUCP4KwYiL-hVpDct24W6IzCwb

#Credit Card Fraud Detection

##Importing the libraries
"""

import pandas as pd
from geopy.distance import geodesic
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf

"""##Loading the Dataset"""

# Load the dataset
file_path = 'fraud_data.csv'
data = pd.read_csv(file_path)

# Step 1: Separate X and y
y = data['is_fraud']
X = data.drop(columns=['is_fraud', 'trans_num'])

"""##Data Preprocessing"""

# Clean the y column
y_cleaned = y.str.extract(r'(\d)').astype(int)
y = y_cleaned[0]

# Step 2: Convert 'trans_date_trans_time' and 'dob' to datetime
X['trans_date_trans_time'] = pd.to_datetime(X['trans_date_trans_time'], format='%d-%m-%Y %H:%M', errors='coerce')
X['dob'] = pd.to_datetime(X['dob'], format='%d-%m-%Y', errors='coerce')

# Step 3: One-hot encoding for categorical variables
X = pd.get_dummies(X, columns=['merchant', 'category', 'city', 'job', 'state'], drop_first=True)

# Step 4: Feature Engineering
X['trans_date'] = X['trans_date_trans_time'].dt.date.apply(lambda x: x.toordinal())
X['trans_time'] = X['trans_date_trans_time'].dt.time.apply(lambda x: x.hour * 3600 + x.minute * 60 + x.second)

X['age'] = (pd.to_datetime('today') - X['dob']).dt.days // 365

X.drop(columns=['trans_date_trans_time', 'dob'], inplace=True)

X['distance'] = X.apply(lambda row: geodesic((row['lat'], row['long']), (row['merch_lat'], row['merch_long'])).km, axis=1)
X.drop(columns=['lat', 'long', 'merch_lat', 'merch_long'], inplace=True)

# Step 5: Scaling/Normalization
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

"""##Splitting the Data into Training and Test Sets"""

# Step 6: Splitting the Data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

"""Building and Training the Artificial Neural Network"""

# Step 7: Build the ANN
ann = tf.keras.models.Sequential()

# Input layer
ann.add(tf.keras.layers.Dense(units=12, activation='relu', input_shape=(X_train.shape[1],)))

# Hidden layers
ann.add(tf.keras.layers.Dense(units=3, activation='relu'))

# Output layer
ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

# Compile the model
ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = ann.fit(X_train, y_train, batch_size=500, epochs=50, validation_data=(X_test, y_test))

"""##Model Evaluation"""

# Evaluate the model on the test set
loss, accuracy = ann.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy:.4f}")

# Predict on the test set
y_pred = (ann.predict(X_test) > 0.5).astype("int32")

# Display classification report and confusion matrix
from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))